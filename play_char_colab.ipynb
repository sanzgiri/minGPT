{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "play_char_colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ede9147f46db4d97bca9cca12526db2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2d070fb30d5c4cf6b67e9a2eef0e3a03",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9704adafc57f4927a9a359ff0ef6c787",
              "IPY_MODEL_df0ab7bc70fe46f5b6167d734bf32b8e"
            ]
          }
        },
        "2d070fb30d5c4cf6b67e9a2eef0e3a03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "9704adafc57f4927a9a359ff0ef6c787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_05c7730dd0c647a0834850defebf9734",
            "_dom_classes": [],
            "description": "Epoch 92:  56%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 34,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 19,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98c7325c3cf441c7ac776f7f5171d020"
          }
        },
        "df0ab7bc70fe46f5b6167d734bf32b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_236b1332b17e47a1928f79ff093aa26f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 19/34 [02:42&lt;02:08,  8.56s/it, loss=0.066, v_num=1]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_558f3cb5ae1d46bb966fd216a72eaaaf"
          }
        },
        "05c7730dd0c647a0834850defebf9734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98c7325c3cf441c7ac776f7f5171d020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "236b1332b17e47a1928f79ff093aa26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "558f3cb5ae1d46bb966fd216a72eaaaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanzgiri/minGPT/blob/master/play_char_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDzrfQrBCKKU",
        "colab_type": "text"
      },
      "source": [
        "## Adapted from https://github.com/williamFalcon/minGPT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuKQX1IoXDeg",
        "colab_type": "text"
      },
      "source": [
        "## Train a character-level GPT on some text data\n",
        "\n",
        "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some shakespear, which we'll get it to predict character-level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnqHMwI4fxty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "outputId": "3de54727-8366-4b59-ea4c-c6c1142f712f"
      },
      "source": [
        "!pip install pytorch_lightning==0.9.0rc16"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_lightning==0.9.0rc16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/7e/38c1c5656f3263df2e48c7243a3ae8f65b75422fd72e2b232213ea4d9664/pytorch_lightning-0.9.0rc16-py3-none-any.whl (388kB)\n",
            "\r\u001b[K     |▉                               | 10kB 14.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30kB 2.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 245kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 256kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 266kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 276kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 286kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 296kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 307kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 317kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 327kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 337kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 348kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 358kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 368kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 378kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 389kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.9.0rc16) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.9.0rc16) (1.18.5)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.9.0rc16) (5.3.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.9.0rc16) (1.6.0+cu101)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.9.0rc16) (0.18.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.9.0rc16) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.9.0rc16) (20.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (0.9.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (49.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (0.34.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (1.17.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (1.31.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (3.2.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytorch_lightning==0.9.0rc16) (2.4.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==0.9.0rc16) (3.1.0)\n",
            "Installing collected packages: pytorch-lightning\n",
            "  Found existing installation: pytorch-lightning 0.8.5\n",
            "    Uninstalling pytorch-lightning-0.8.5:\n",
            "      Successfully uninstalled pytorch-lightning-0.8.5\n",
            "Successfully installed pytorch-lightning-0.9.0rc16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pytorch_lightning"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZoYHEragiB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c9b6367-9481-43df-90ca-1f9ac100b71a"
      },
      "source": [
        "!git clone https://github.com/williamFalcon/minGPT"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'minGPT' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzJ4J7rGgvQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bff4b175-64cb-4b65-a14c-376be905f172"
      },
      "source": [
        "%cd minGPT"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/minGPT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEW8HjyeXDe7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "083b467e-8167-4e01-c7b8-c803719a5755"
      },
      "source": [
        "# make deterministic\n",
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(42)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpPg8n1Ufq7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LRCsq-nf-s6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = list(set(data))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
        "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
        "        chunk = self.data[i:i+self.block_size+1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp_C3mXrgAzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "block_size = 128 # spatial extent of the model for its context"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG-un31hgDyo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2cf670b4-7315-4b98-b81d-0f61e58a38aa"
      },
      "source": [
        "# download tiny shakespeare input text\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-20 06:18:27--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-08-20 06:18:27 (9.54 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bhKFmLkgFtR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9155f6d-3b24-4018-d8f6-daa709aed7df"
      },
      "source": [
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, num_workers=4)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydc6IcKFgKog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mingpt.model import GPT\n",
        "model = GPT(vocab_size=train_dataset.vocab_size, \n",
        "            block_size=train_dataset.block_size,\n",
        "            n_layer=8, \n",
        "            n_head=8, \n",
        "            n_embd=512, \n",
        "            learning_rate=6e-4)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7bpEFx0gb1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "ede9147f46db4d97bca9cca12526db2d",
            "2d070fb30d5c4cf6b67e9a2eef0e3a03",
            "9704adafc57f4927a9a359ff0ef6c787",
            "df0ab7bc70fe46f5b6167d734bf32b8e",
            "05c7730dd0c647a0834850defebf9734",
            "98c7325c3cf441c7ac776f7f5171d020",
            "236b1332b17e47a1928f79ff093aa26f",
            "558f3cb5ae1d46bb966fd216a72eaaaf"
          ]
        },
        "outputId": "1dfda394-9d58-4f82-eaec-c053299a2dd6"
      },
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from mingpt.lr_decay import LearningRateDecayCallback\n",
        "\n",
        "# scheduler\n",
        "lr_decay = LearningRateDecayCallback(learning_rate=6e-4, warmup_tokens=512*20,\n",
        "                                    final_tokens=00*len(train_dataset)*block_size)\n",
        "\n",
        "trainer = Trainer(gpus=1, precision=16, max_epochs=500,\n",
        "                  gradient_clip_val=1.0, \n",
        "                  callbacks=[lr_decay], \n",
        "                  progress_bar_refresh_rate=1, \n",
        "                  row_log_interval=1)\n",
        "\n",
        "trainer.fit(model, train_loader)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n",
            "Using native 16bit precision.\n",
            "\n",
            "  | Name    | Type       | Params\n",
            "---------------------------------------\n",
            "0 | tok_emb | Embedding  | 33 K  \n",
            "1 | drop    | Dropout    | 0     \n",
            "2 | blocks  | Sequential | 25 M  \n",
            "3 | ln_f    | LayerNorm  | 1 K   \n",
            "4 | head    | Linear     | 33 K  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ede9147f46db4d97bca9cca12526db2d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: \n",
            "                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the\n",
            "                    'monitor' key of ModelCheckpoint has no effect.\n",
            "                    Remove ModelCheckpoint(monitor='loss) to fix')\n",
            "                \n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLiAUVJjg12h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "2994b20d-ac5c-4d4b-8720-3fc27b361487"
      },
      "source": [
        "# alright, let's sample some character-level shakespear\n",
        "from mingpt.utils import sample\n",
        "\n",
        "context = \"O God, I code but\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(model.device)\n",
        "y = sample(model, x, 1000, temperature=0.9, sample=True, top_k=5)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O God, I code but me and do impor From the profund.\n",
            "\n",
            "WARWICK:\n",
            "Now, Who not obe?\n",
            "\n",
            "BIONDELLO:\n",
            "I will not hear the state, my heart, lords,\n",
            "That she is at Volscience, to state teat,\n",
            "To getters to the Vold and ender at holy some to\n",
            "Rathut have I wish even your lander's likeng a talkwing?\n",
            "\n",
            "First Seenator:\n",
            "I begging to you, and ye'e to more,\n",
            "And show to ever pass.\n",
            "\n",
            "LUCIO:\n",
            "Is she is too sea, which of we the hat bows\n",
            "ISABELLA:\n",
            "Under like to she may in this dand count-sellor,\n",
            "The bears of with even, we may so pirit,\n",
            "Read the world Cominius even, let me she'r not for't.\n",
            "\n",
            "BRUTUS:\n",
            "Verkener for the for this, of the devil:\n",
            "What would be solr, in this lawul form.\n",
            "\n",
            "Messenger:\n",
            "Pet now, my heart, but shall is liefe ames:\n",
            "And therefore, in thou given liest, wheret for his larks.\n",
            "\n",
            "LARTIUS:\n",
            "Wherein she lieutenand or shorless on my son,\n",
            "In a lready me to bed so, of infaict, I would tpose thee\n",
            "May with thou have little wof to statury little him.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Why, 'tis thou like: what's to 'thou livest!\n",
            "\n",
            "LADY CAPULET:\n",
            "Vert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE1F1TOrB7xg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWxA-gDZg5ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}